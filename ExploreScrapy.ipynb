{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f517517-8647-436b-918a-2637c5925a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import rich\n",
    "from scrapy import Selector\n",
    "import requests\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4992b2d8-38c9-4ff9-ab8d-af8445ba2c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B')\n",
    "\n",
    "def count_tokens(text):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d94d9-275a-451a-beca-56868667bfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_collection(url, api_key, knowledge_list):\n",
    "    # Check if collections already exists\n",
    "    full_url = url + \":8080/api/v1/knowledge/list\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    response = requests.get(url=full_url, headers=headers)\n",
    "    # Exit early if api call fails\n",
    "    if response.status_code not in range(200,299):\n",
    "        error_message = f\"Recieved Non-Successful Status Code({response.status_code}), and message :{response.text}\"\n",
    "        print(error_message)\n",
    "        return error_message\n",
    "\n",
    "    # Parse response for list of exiting knowledges. See which are missing\n",
    "    response_json = response.json()\n",
    "    confirmed_knowledges = []\n",
    "    print(json.dumps(response_json, indent=2))\n",
    "    for knowledge in response_json:\n",
    "        confirmed_knowledges.append(knowledge[\"name\"])\n",
    "    missing_knowledges = list(set(knowledge_list) - set(confirmed_knowledges))\n",
    "    print(f\"Aleady existing knowledges: {confirmed_knowledges}\")\n",
    "    if missing_knowledges:\n",
    "        print(f\"Missing knowledges to create: {', '.join(missing_knowledges)}\")\n",
    "    else:\n",
    "        print(\"There are no knowledges to create\")\n",
    "        return\n",
    "\n",
    "    # Send Create Knowledge API call for each missing knowledge\n",
    "    for missing_knowledge in missing_knowledges:\n",
    "        create_knowledge_url = url + \":8080/api/v1/knowledge/create\"\n",
    "        data = {\n",
    "            \"name\": missing_knowledge,\n",
    "            \"description\": f\"Create fextralife's '{missing_knowledge}' knowledge partition\",\n",
    "            \"access_control\": {\n",
    "                \"public\": True,\n",
    "            },\n",
    "        }\n",
    "        print(f\"Attempting to create knowledge: {missing_knowledge}\")\n",
    "        create_response = requests.post(url=create_knowledge_url, json=data, headers=headers)\n",
    "        if create_response.status_code not in range(200,299):\n",
    "            error_message = f\"Recieved Non-Successful Status Code({create_response.status_code}), and message :{create_response.text}\"\n",
    "            print(error_message)\n",
    "            return error_message\n",
    "\n",
    "        print(f\"Creation Succeeded for knowledge: {missing_knowledge}\")\n",
    "        print(f\"Confirmation response: {json.dumps(create_response.json(), indent=2)}\")\n",
    "\n",
    "url = 'http://localhost'\n",
    "api_key = '<pagste your openw web ui api key here>'\n",
    "knowledge_list = ['Weapons', 'Armor', 'Items', 'Decorations', 'Misc']\n",
    "create_all_collection(url, api_key, knowledge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56069268-1e63-4a16-88c4-089be7327ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./wikiproject/output/fextralife-monsterhunterwildswiki.jsonl') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "    \n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a701c9-a5fd-4e5d-8437-5c616f4de92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['doc_filepath'] = \"./wikiproject/output/documents/\" + (df['breadcrumb'].str.replace(\"/\", \"-\") + \"-\" + df['title']).str.replace(\"/\", \"-\").str.strip(\"-\") + \".txt\"\n",
    "example_filepath = df['doc_filepath'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab9b2bf-8234-4b15-9c41-cf231a7ec1af",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upload_file_to_openwebui_knowledge(url, api_key, filepath):\n",
    "    #\n",
    "    full_url = url + \":8080/api/v1/files/\"\n",
    "    files = {'file': open(filepath, 'rb')}\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {api_key}',\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "\n",
    "    upload_response = requests.post(url=full_url, headers=headers, files=files)\n",
    "    if upload_response.status_code not in range(200,299):\n",
    "        print(\"Oopsies, the upload failed!\")\n",
    "        print(upload_response.text)\n",
    "        return\n",
    "    upload_response_json = upload_response.json()\n",
    "    print(json.dumps(upload_response_json, indent=2))\n",
    "\n",
    "    \n",
    "\n",
    "    add_to_knowledge_response = requests.\n",
    "    \n",
    "url = 'http://localhost'\n",
    "api_key = \"<paste your open web ui api key here>\"\n",
    "upload_file_to_openwebui_knowledge(\"http://localhost\", api_key, example_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8249d325-5708-4786-975a-d19ee17bebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_to_openwebui(url, api_key):\n",
    "    full_url = url + \":8080/api/v1/files/\"\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {api_key}',\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "\n",
    "    filelist_response = requests.get(url=full_url, headers=headers)\n",
    "    if filelist_response.status_code not in range(200,299):\n",
    "        print(\"Oopsies, the upload failed!\")\n",
    "        print(filelist_response.text)\n",
    "        return\n",
    "    print(json.dumps(filelist_response.json(), indent=2))\n",
    "    \n",
    "url = 'http://localhost'\n",
    "api_key = \"<paste your open web ui api key here>\"\n",
    "get_files_to_openwebui(\"http://localhost\", api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384a9b4b-5a37-4b27-8a33-748c10074b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_list_get (l, idx, default):\n",
    "  try:\n",
    "    return l[idx]\n",
    "  except IndexError:\n",
    "    return default\n",
    "      \n",
    "df['first_breadcrumb'] = df['breadcrumb'].apply(lambda x: safe_list_get(x.split('/'), 1, 0)) \n",
    "df['second_breadcrumb'] = df['breadcrumb'].apply(lambda x: safe_list_get(x.split('/'), 2, 0)).astype(str)\n",
    "df['combined_breadcrumb'] = df['first_breadcrumb'] + df['second_breadcrumb']\n",
    "\n",
    "def count_tokens_in_wiki_content(series):\n",
    "    return count_tokens(series.str.cat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0074a5c-6bde-4b63-ae61-3801ca5c8388",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_breadcrumb_list = df['first_breadcrumb'].unique().tolist()\n",
    "second_breadcrumb_list = df['second_breadcrumb'].unique().tolist()\n",
    "\n",
    "first_breadcrumb_count = dict.fromkeys(first_breadcrumb_list, 0)\n",
    "second_breadcrumb_count = dict.fromkeys(second_breadcrumb_list, 0)\n",
    "for index, row in df.iterrows():\n",
    "    first_breadcrumb_count[row['first_breadcrumb']] += count_tokens(row['wiki_content'])\n",
    "    second_breadcrumb_count[row['second_breadcrumb']] += count_tokens(row['wiki_content'])\n",
    "\n",
    "print(json.dumps(first_breadcrumb_count, indent=2))\n",
    "print(json.dumps(second_breadcrumb_count, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291436a4-9468-43f8-91d7-45383099b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, val in first_breadcrumb_count.items():\n",
    "    first_breadcrumb_count[idx] = first_breadcrumb_count[idx] / 24679933.0 * 100\n",
    "    \n",
    "for idx, val in second_breadcrumb_count.items():\n",
    "    second_breadcrumb_count[idx] = second_breadcrumb_count[idx] / 24679933.0 * 100\n",
    "print(json.dumps(first_breadcrumb_count, indent=2))\n",
    "print(json.dumps(second_breadcrumb_count, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ba8a5-3765-4956-9e19-fb4d54c26b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['first_breadcrumb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3bc4c2-25ab-4295-96d6-5ad0be22fcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0c6861-d858-455e-80b3-215cff5a9d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b8722e-7714-4e50-8edb-1f59f8f2c4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens(df['wiki_content'].str.cat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0b30ba-d7c4-4299-8d64-670df778a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_breadcrumb(url, html_content):\n",
    "    sel = Selector(text=html_content)\n",
    "    url_end_route = url.split(\"/\")[-1]\n",
    "    breadcrumb_tags = \"/\" + \"/\".join([x for x in sel.css('div.breadcrumb-wrapper a::text').getall() if x != '+']) + \"/\" + url_end_route\n",
    "    if breadcrumb_tags == \"\":\n",
    "        return \"/\" + url_end_route\n",
    "    return breadcrumb_tags\n",
    "\n",
    "get_breadcrumb(df.url[10], df.content[10])\n",
    "print(df.url[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0997bc-24f2-4ad6-903a-65498615cbc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scrapy import Selector\n",
    "\n",
    "sel = Selector(text=df.content[11])\n",
    "html_node = sel.css('html')\n",
    "\n",
    "wiki_tables = html_node.xpath('//table[@class=\"wiki_table\"]').getall()\n",
    "\n",
    "wiki_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd78928-865a-4670-8edc-fc9c112f51ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_table_with_selector(html):\n",
    "    sel = Selector(text=html)\n",
    "    tables = sel.xpath('//table[@class=\"wiki_table\"]').getall()\n",
    "\n",
    "    normalized_data = []\n",
    "\n",
    "    for table_html in tables:\n",
    "        table_sel = Selector(text=table_html)\n",
    "        \n",
    "        # Extract headers from <thead> if present, else first <tr>\n",
    "        headers = []\n",
    "        thead = table_sel.xpath('./thead')\n",
    "        if thead:\n",
    "            headers = thead.xpath('.//th//text()').getall()\n",
    "            headers = [h.strip() for h in headers if h.strip()]\n",
    "        else:\n",
    "            first_tr = table_sel.xpath('.//tr')[0]\n",
    "            headers = first_tr.xpath('./th//text() | ./td//text()').getall()\n",
    "            headers = [h.strip() for h in headers if h.strip()]\n",
    "\n",
    "        # Extract ros, skipping header row if no thead\n",
    "        if thead:\n",
    "            rows = table_sel.xpath('./tbody/tr')\n",
    "        else:\n",
    "            rows = table_sel.xpath('.//tr')[1:] # skip first header row\n",
    "\n",
    "        data = []\n",
    "        max_len = len(headers)\n",
    "\n",
    "        for row in rows:\n",
    "            cells = row.xpath('./th | ./td')\n",
    "            row_data = []\n",
    "            for cell in cells:\n",
    "                # Check for nested table inside cell\n",
    "                nested_table = cell.xpath('.//table')\n",
    "                if nested_table:\n",
    "                    nested_html = nested_table.get()\n",
    "                    nested_html = parser_table_with_selector(nested_html)\n",
    "                    row_data.append(nested_data)\n",
    "                else: \n",
    "                    # Prefer alt or title if image present\n",
    "                    img = cell.xpath('.//img')\n",
    "                    if img:\n",
    "                        alt = img.xpath('./@alt').get()\n",
    "                        title = img.xpath('./@title').get()\n",
    "                        text = alt or title or cell.xpath('string(.)').get()\n",
    "                    else:\n",
    "                        text = cell.xpath('string(.)').get()\n",
    "                    row_data.append(text.strip() if text else '')\n",
    "\n",
    "            max_len = max(max_len, len(row_data))\n",
    "            data.append(row_data)\n",
    "\n",
    "        # Pad headers or rows to max_len\n",
    "        if len(headers) < max_len:\n",
    "            headers += [f\"Extra_{i}\" for i in range(max_len - len(headers))]\n",
    "\n",
    "        for r in data:\n",
    "            r += [''] * (max_len - len(r))\n",
    "            normalized_data.append(dict(zip(headers, r)))\n",
    "\n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "table_json = parse_table_with_selector(df.content[11])\n",
    "import json\n",
    "print(type(json.dumps(table_json,indent=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2200057f-0ac1-4cee-9b88-ec8e17eb4f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_content(html_content):\n",
    "    sel = Selector(text=html_content)\n",
    "    wikicontent = (\" \".join([x.strip() for x in sel.xpath('//div[@id=\"wiki-content-block\"]//text()').getall()])).replace('\\xa0', ' ')\n",
    "    return wikicontent\n",
    "\n",
    "get_wiki_content(df.content[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a9565c-d230-4792-adc2-2f60f6971208",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from rich.tree import Tree\n",
    "from rich import print\n",
    "\n",
    "def createPrintableTree(df):\n",
    "    tree = Tree(\"monsterhunterwilds.wiki.fextralife.com:root\")\n",
    "    nodes = {}\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        path = get_breadcrumb(row['url'], row['content']).replace('\\xa0', ' ')\n",
    "        parts = [p for p in path.split('/') if p]\n",
    "        parent = tree\n",
    "        partial = \"\"\n",
    "        for part in parts:\n",
    "            partial += \"/\" + part\n",
    "            if partial not in nodes:\n",
    "                nodes[partial] = parent.add(part)\n",
    "            parent = nodes[partial]\n",
    "    \n",
    "    return tree\n",
    "\n",
    "printableTree = createPrintableTree(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a7dd18-bb89-41c9-8d2c-04dea2b4e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "console = Console(record=True)\n",
    "console.print(printableTree)\n",
    "output = console.export_text()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e22d11-6f48-4bd3-816d-fc60d594f1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['breadcrumb&title'] = \"\"\n",
    "for index, row in df.iterrows():\n",
    "    path = get_breadcrumb(row['url'], row['content']).replace('\\xa0', ' ')\n",
    "    df.at[index, 'breadcrumb&title'] = path\n",
    "\n",
    "df['breadcrumb&title'].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e55fb1-b558-42dd-b03e-692fe80fa2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.url.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00da7783-013b-44ce-90fd-7bf48189fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49056824-2e15-4a6a-a00f-4278a937a5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
